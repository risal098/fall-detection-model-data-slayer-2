{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10200117,"sourceType":"datasetVersion","datasetId":6303014},{"sourceId":10264378,"sourceType":"datasetVersion","datasetId":6350028}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import ResNet152V2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport torch\nfrom PIL import Image\nimport cv2\n\n\n# Pastikan menggunakan GPU jika tersedia\nif tf.config.list_physical_devices('GPU'):\n    print(\"GPU Tersedia\")\nelse:\n    print(\"GPU Tidak Tersedia, menggunakan CPU\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T03:41:40.005245Z","iopub.execute_input":"2024-12-25T03:41:40.005892Z","iopub.status.idle":"2024-12-25T03:41:40.012706Z","shell.execute_reply.started":"2024-12-25T03:41:40.005862Z","shell.execute_reply":"2024-12-25T03:41:40.011886Z"}},"outputs":[{"name":"stdout","text":"GPU Tersedia\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"# Path dataset\nbase_path = '/kaggle/input/human-fall-datasets/train'\ntest_path = '/kaggle/input/human-fall-datasets/test'\n\n# Fungsi untuk membaca dataset\ndef process_directory(base_path, label_map):\n    data = []\n    for subject in os.listdir(base_path):\n        subject_path = os.path.join(base_path, subject)\n        if os.path.isdir(subject_path):\n            for category in os.listdir(subject_path):\n                category_path = os.path.join(subject_path, category)\n                label = label_map.get(category, None)\n                if label is not None:\n                    for subcategory in os.listdir(category_path):\n                        subcategory_path = os.path.join(category_path, subcategory)\n                        for filename in os.listdir(subcategory_path):\n                            if filename.endswith('.jpg'):\n                                file_path = os.path.join(subcategory_path, filename)\n                                data.append({\n                                    \"subject\": subject,\n                                    \"category\": category,\n                                    \"subcategory\": subcategory,\n                                    \"file_path\": file_path,\n                                    \"label\": label\n                                })\n    return data\n#encode kategori class output\nlabel_map = {\n    \"fall\": 1,\n    \"non_fall\": 0\n}\n\n\n# Proses data\ntrain_data = process_directory(base_path, label_map)\ndf = pd.DataFrame(train_data)\ntest_data = []\nfor filename in os.listdir(test_path):\n    if filename.endswith('.jpg'):\n        file_path = os.path.join(test_path, filename)\n        test_data.append({\"id\": filename, \"file_path\": file_path})\ntest_df = pd.DataFrame(test_data)\n\n\n# load yolov5 model\nmodelYolo = torch.hub.load('ultralytics/yolov5', 'yolov5s')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T03:41:40.013920Z","iopub.execute_input":"2024-12-25T03:41:40.014148Z","iopub.status.idle":"2024-12-25T03:41:40.795435Z","shell.execute_reply.started":"2024-12-25T03:41:40.014129Z","shell.execute_reply":"2024-12-25T03:41:40.794545Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# Split data\nX = df['file_path']\ny = df['label']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Preprocessing gambar\ndef yolo_crop(image_path):\n    # Decode bytes to string if needed\n    if isinstance(image_path, bytes):\n        image_path = image_path.decode('utf-8')\n    \n    # Perform YOLO detection\n    results = modelYolo(Image.open(image_path))\n    detections = results.xyxy[0]  # [xmin, ymin, xmax, ymax, confidence, class]\n    \n    for *box, conf, cls in detections:\n        if int(cls) == 0:  # Class 0 corresponds to 'person' in the COCO dataset\n            xmin, ymin, xmax, ymax = map(int, box)\n            # Open image and crop based on detection\n            image = Image.open(image_path)\n            cropped_image = image.crop((xmin, ymin, xmax, ymax))\n            return cropped_image\n\n    # Return the original image if no detection\n    return Image.open(image_path)\n\n\n# Image preprocessing function\ndef preprocess_image(image_path, img_size=(224, 224)):\n    # Use YOLO to crop the image\n    cropped_image = yolo_crop(image_path)\n    \n    # Convert PIL image to a TensorFlow-compatible format\n    cropped_image = cropped_image.convert('RGB')  # Ensure 3 channels\n    cropped_image = cropped_image.resize(img_size)  # Resize to target dimensions\n    img_array = tf.keras.preprocessing.image.img_to_array(cropped_image)\n    \n    # Preprocess using ResNetV2 preprocessing\n    img_array = tf.keras.applications.resnet_v2.preprocess_input(img_array)\n    return img_array\n\n# Wrapper to use with TensorFlow dataset\ndef preprocess_image_wrapper(image_path, label, img_size=(224, 224)):\n    img = tf.numpy_function(preprocess_image, [image_path], tf.float32)\n    img.set_shape((img_size[0], img_size[1], 3))  # Explicitly set the shape\n    return img, label\n\n\n\n# Data generator for training\ntrain_gen = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntrain_gen = train_gen.map(lambda x, y: preprocess_image_wrapper(x, y))\ntrain_gen = train_gen.batch(16).prefetch(tf.data.AUTOTUNE)\n\n# Data generator for validation\nval_gen = tf.data.Dataset.from_tensor_slices((X_val, y_val))\nval_gen = val_gen.map(lambda x, y: preprocess_image_wrapper(x, y))\nval_gen = val_gen.batch(16).prefetch(tf.data.AUTOTUNE)\n\nprint(type(val_gen))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T03:41:59.659967Z","iopub.execute_input":"2024-12-25T03:41:59.660262Z","iopub.status.idle":"2024-12-25T03:41:59.783639Z","shell.execute_reply.started":"2024-12-25T03:41:59.660230Z","shell.execute_reply":"2024-12-25T03:41:59.782615Z"}},"outputs":[{"name":"stdout","text":"<class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Arsitektur Model","metadata":{}},{"cell_type":"code","source":"# Model\ndef create_model():\n    base_model = ResNet152V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    \n    # Unfreeze beberapa layer terakhir\n    for layer in base_model.layers[-30:]:\n        layer.trainable = True\n    \n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(1, activation='sigmoid')(x)\n    \n    return Model(base_model.input, outputs)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nprint(\"w\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T03:41:59.784475Z","iopub.execute_input":"2024-12-25T03:41:59.784708Z","iopub.status.idle":"2024-12-25T03:41:59.795910Z","shell.execute_reply.started":"2024-12-25T03:41:59.784688Z","shell.execute_reply":"2024-12-25T03:41:59.795267Z"}},"outputs":[{"name":"stdout","text":"w\n","output_type":"stream"}],"execution_count":7},{"cell_type":"raw","source":"# Training Model Rongawi kita","metadata":{}},{"cell_type":"code","source":"#menghilangkan warning untuk model yolo\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Callbacks\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=5,\n        min_lr=1e-6,\n        verbose=1\n    ),\n    ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy')\n]\n\n# Ensemble models\ndef train_ensemble(n_models=3):\n    models = []\n    histories = []\n    \n    for i in range(n_models):\n        print(f\"\\nTraining Model {i+1}/{n_models}\")\n        model = create_model()\n        \n        # Buat optimizer baru untuk setiap model\n        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n        \n        model.compile(\n            optimizer=optimizer,\n            loss='binary_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        history = model.fit(\n            train_gen,\n            validation_data=val_gen,\n            epochs=80,\n            callbacks=callbacks\n        )\n        \n        models.append(model)\n        histories.append(history)\n    \n    return models, histories\n\n# Train ensemble\nmodels, histories = train_ensemble()\n\n# Prediksi ensemble\ndef ensemble_predict(models, data):\n    predictions = []\n    for model in models:\n        pred = model.predict(data)\n        predictions.append(pred)\n    \n    # Rata-rata prediksi\n    ensemble_pred = np.mean(predictions, axis=0)\n    return ensemble_pred\n\n# Evaluasi model ensemble\nval_predictions = ensemble_predict(models, val_gen)\nval_pred_classes = (val_predictions > 0.5).astype(int).flatten()\n\nprint(\"\\nLaporan Klasifikasi:\")\nprint(classification_report(y_val, val_pred_classes))\n\n# Plot confusion matrix\ncm = confusion_matrix(y_val, val_pred_classes)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T03:41:59.796652Z","iopub.execute_input":"2024-12-25T03:41:59.796924Z","iopub.status.idle":"2024-12-25T06:00:56.432300Z","shell.execute_reply.started":"2024-12-25T03:41:59.796892Z","shell.execute_reply":"2024-12-25T06:00:56.431281Z"}},"outputs":[{"name":"stdout","text":"\nTraining Model 1/3\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m234545216/234545216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nEpoch 1/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 1s/step - accuracy: 0.8998 - loss: 18.8645 - val_accuracy: 0.9942 - val_loss: 13.1367 - learning_rate: 1.0000e-04\nEpoch 2/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 821ms/step - accuracy: 0.9899 - loss: 11.6163 - val_accuracy: 0.9953 - val_loss: 7.5927 - learning_rate: 1.0000e-04\nEpoch 3/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 816ms/step - accuracy: 0.9958 - loss: 6.5955 - val_accuracy: 1.0000 - val_loss: 4.0862 - learning_rate: 1.0000e-04\nEpoch 4/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 794ms/step - accuracy: 0.9993 - loss: 3.5044 - val_accuracy: 1.0000 - val_loss: 2.0787 - learning_rate: 1.0000e-04\nEpoch 5/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 790ms/step - accuracy: 1.0000 - loss: 1.7635 - val_accuracy: 1.0000 - val_loss: 1.0098 - learning_rate: 1.0000e-04\nEpoch 6/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 792ms/step - accuracy: 1.0000 - loss: 0.8508 - val_accuracy: 1.0000 - val_loss: 0.4760 - learning_rate: 1.0000e-04\nEpoch 7/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 796ms/step - accuracy: 1.0000 - loss: 0.3996 - val_accuracy: 1.0000 - val_loss: 0.2210 - learning_rate: 1.0000e-04\nEpoch 8/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 805ms/step - accuracy: 1.0000 - loss: 0.1860 - val_accuracy: 1.0000 - val_loss: 0.1032 - learning_rate: 1.0000e-04\nEpoch 9/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 796ms/step - accuracy: 1.0000 - loss: 0.0875 - val_accuracy: 1.0000 - val_loss: 0.0502 - learning_rate: 1.0000e-04\nEpoch 10/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 793ms/step - accuracy: 1.0000 - loss: 0.0437 - val_accuracy: 1.0000 - val_loss: 0.0270 - learning_rate: 1.0000e-04\nEpoch 11/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 787ms/step - accuracy: 1.0000 - loss: 0.0246 - val_accuracy: 1.0000 - val_loss: 0.0170 - learning_rate: 1.0000e-04\nEpoch 12/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 793ms/step - accuracy: 1.0000 - loss: 0.0165 - val_accuracy: 1.0000 - val_loss: 0.0128 - learning_rate: 1.0000e-04\nEpoch 13/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 787ms/step - accuracy: 1.0000 - loss: 0.0134 - val_accuracy: 1.0000 - val_loss: 0.0116 - learning_rate: 1.0000e-04\nEpoch 14/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 793ms/step - accuracy: 0.9628 - loss: 0.1537 - val_accuracy: 0.9511 - val_loss: 0.1859 - learning_rate: 1.0000e-04\nEpoch 15/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 788ms/step - accuracy: 0.9780 - loss: 0.1206 - val_accuracy: 0.9756 - val_loss: 0.0890 - learning_rate: 1.0000e-04\nEpoch 16/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 792ms/step - accuracy: 0.9936 - loss: 0.0477 - val_accuracy: 0.9919 - val_loss: 0.0446 - learning_rate: 1.0000e-04\nEpoch 17/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 785ms/step - accuracy: 0.9969 - loss: 0.0301 - val_accuracy: 0.9965 - val_loss: 0.0277 - learning_rate: 1.0000e-04\nEpoch 18/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677ms/step - accuracy: 0.9961 - loss: 0.0348\nEpoch 18: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 788ms/step - accuracy: 0.9961 - loss: 0.0348 - val_accuracy: 0.9953 - val_loss: 0.0283 - learning_rate: 1.0000e-04\nEpoch 19/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 786ms/step - accuracy: 0.9994 - loss: 0.0207 - val_accuracy: 1.0000 - val_loss: 0.0177 - learning_rate: 2.0000e-05\nEpoch 20/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 783ms/step - accuracy: 1.0000 - loss: 0.0170 - val_accuracy: 1.0000 - val_loss: 0.0161 - learning_rate: 2.0000e-05\nEpoch 21/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 782ms/step - accuracy: 1.0000 - loss: 0.0155 - val_accuracy: 1.0000 - val_loss: 0.0150 - learning_rate: 2.0000e-05\nEpoch 22/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 778ms/step - accuracy: 1.0000 - loss: 0.0144 - val_accuracy: 1.0000 - val_loss: 0.0141 - learning_rate: 2.0000e-05\nEpoch 23/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671ms/step - accuracy: 1.0000 - loss: 0.0136\nEpoch 23: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 778ms/step - accuracy: 1.0000 - loss: 0.0136 - val_accuracy: 1.0000 - val_loss: 0.0133 - learning_rate: 2.0000e-05\n\nTraining Model 2/3\nEpoch 1/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 1s/step - accuracy: 0.8972 - loss: 18.8386 - val_accuracy: 0.9907 - val_loss: 12.8940 - learning_rate: 1.0000e-04\nEpoch 2/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 784ms/step - accuracy: 0.9910 - loss: 11.2917 - val_accuracy: 0.9907 - val_loss: 7.1936 - learning_rate: 1.0000e-04\nEpoch 3/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 781ms/step - accuracy: 0.9922 - loss: 6.2071 - val_accuracy: 0.9977 - val_loss: 3.7606 - learning_rate: 1.0000e-04\nEpoch 4/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 783ms/step - accuracy: 0.9995 - loss: 3.2054 - val_accuracy: 0.9977 - val_loss: 1.8695 - learning_rate: 1.0000e-04\nEpoch 5/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 780ms/step - accuracy: 1.0000 - loss: 1.5689 - val_accuracy: 0.9977 - val_loss: 0.8886 - learning_rate: 1.0000e-04\nEpoch 6/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 804ms/step - accuracy: 1.0000 - loss: 0.7371 - val_accuracy: 0.9988 - val_loss: 0.4110 - learning_rate: 1.0000e-04\nEpoch 7/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 783ms/step - accuracy: 1.0000 - loss: 0.3373 - val_accuracy: 0.9988 - val_loss: 0.1900 - learning_rate: 1.0000e-04\nEpoch 8/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 786ms/step - accuracy: 1.0000 - loss: 0.1532 - val_accuracy: 0.9988 - val_loss: 0.0911 - learning_rate: 1.0000e-04\nEpoch 9/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 781ms/step - accuracy: 1.0000 - loss: 0.0712 - val_accuracy: 0.9988 - val_loss: 0.0477 - learning_rate: 1.0000e-04\nEpoch 10/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 800ms/step - accuracy: 1.0000 - loss: 0.0358 - val_accuracy: 0.9988 - val_loss: 0.0300 - learning_rate: 1.0000e-04\n\nTraining Model 3/3\nEpoch 1/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 1s/step - accuracy: 0.8950 - loss: 18.9034 - val_accuracy: 0.9895 - val_loss: 13.2147 - learning_rate: 1.0000e-04\nEpoch 2/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 793ms/step - accuracy: 0.9894 - loss: 11.7132 - val_accuracy: 0.9977 - val_loss: 7.7123 - learning_rate: 1.0000e-04\nEpoch 3/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 782ms/step - accuracy: 0.9975 - loss: 6.7045 - val_accuracy: 0.9942 - val_loss: 4.1789 - learning_rate: 1.0000e-04\nEpoch 4/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 790ms/step - accuracy: 0.9993 - loss: 3.5704 - val_accuracy: 1.0000 - val_loss: 2.1181 - learning_rate: 1.0000e-04\nEpoch 5/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 792ms/step - accuracy: 0.9997 - loss: 1.7972 - val_accuracy: 0.9988 - val_loss: 1.0340 - learning_rate: 1.0000e-04\nEpoch 6/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 790ms/step - accuracy: 1.0000 - loss: 0.8718 - val_accuracy: 1.0000 - val_loss: 0.4895 - learning_rate: 1.0000e-04\nEpoch 7/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 799ms/step - accuracy: 1.0000 - loss: 0.4136 - val_accuracy: 0.9988 - val_loss: 0.2363 - learning_rate: 1.0000e-04\nEpoch 8/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 801ms/step - accuracy: 1.0000 - loss: 0.1947 - val_accuracy: 0.9988 - val_loss: 0.1114 - learning_rate: 1.0000e-04\nEpoch 9/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 804ms/step - accuracy: 1.0000 - loss: 0.0923 - val_accuracy: 0.9988 - val_loss: 0.0549 - learning_rate: 1.0000e-04\nEpoch 10/80\n\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 797ms/step - accuracy: 1.0000 - loss: 0.0454 - val_accuracy: 0.9988 - val_loss: 0.0313 - learning_rate: 1.0000e-04\n\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 555ms/step\n\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 545ms/step\n\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 545ms/step\n\nLaporan Klasifikasi:\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      1.00       550\n           1       1.00      0.99      1.00       309\n\n    accuracy                           1.00       859\n   macro avg       1.00      1.00      1.00       859\nweighted avg       1.00      1.00      1.00       859\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Evaluasi Model","metadata":{}},{"cell_type":"code","source":"# Plot training history\ndef plot_training_history(histories):\n    plt.figure(figsize=(12, 4))\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    for i, history in enumerate(histories):\n        plt.plot(history.history['accuracy'], label=f'Train {i+1}')\n        plt.plot(history.history['val_accuracy'], label=f'Val {i+1}')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.subplot(1, 2, 2)\n    for i, history in enumerate(histories):\n        plt.plot(history.history['loss'], label=f'Train {i+1}')\n        plt.plot(history.history['val_loss'], label=f'Val {i+1}')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_training_history(histories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T06:00:56.435490Z","iopub.execute_input":"2024-12-25T06:00:56.435908Z","iopub.status.idle":"2024-12-25T06:00:56.808317Z","shell.execute_reply.started":"2024-12-25T06:00:56.435869Z","shell.execute_reply":"2024-12-25T06:00:56.807478Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# File Submission dan Testing","metadata":{}},{"cell_type":"code","source":"def preprocess_image(image_path, img_size=(224, 224)):\n    # YOLO crop wrapper for TensorFlow\n    def yolo_crop_wrapper(image_path):\n    # Decode from bytes to string if necessary\n        if isinstance(image_path, np.ndarray):\n            image_path = image_path.item()  # Get the string from NumPy scalar\n        \n        # Perform YOLO cropping\n        cropped_image = yolo_crop(image_path)\n        \n        # Resize the cropped image to the target size\n        cropped_image = cropped_image.resize((224, 224))  # Ensure (128, 128) dimensions\n        \n        # Convert to NumPy array with float32 type\n        cropped_image = np.array(cropped_image, dtype=np.float32)\n        return cropped_image\n\n\n    # Use tf.numpy_function to call the Python-based function\n    img_array = tf.numpy_function(yolo_crop_wrapper, [image_path], tf.float32)\n    img_array = tf.reshape(img_array, (*img_size, 3))  # Explicitly set shape\n\n    # Preprocess using ResNetV2 preprocessing\n    img_array = tf.keras.applications.resnet_v2.preprocess_input(img_array)\n    return img_array\n\ndef preprocess_image_wrapper2(image_path, img_size=(224, 224)):\n    img = tf.numpy_function(preprocess_image, [image_path], tf.float32)\n    img.set_shape((img_size[0], img_size[1], 3))  # Explicitly set the shape\n    return img\n\n# Dataset for test images\ntest_gen = tf.data.Dataset.from_tensor_slices(test_df['file_path'].values)\ntest_gen = test_gen.map(preprocess_image_wrapper2)\ntest_gen = test_gen.batch(16).prefetch(tf.data.AUTOTUNE)\n\n# Run predictions\ntest_predictions = ensemble_predict(models, test_gen)\n\n# Post-process predictions and save results\ntest_df['label'] = (test_predictions > 0.5).astype(int)\ntest_df[['id', 'label']].to_csv('/kaggle/working/predictions1.csv', index=False)\nprint(\"Predictions saved yes.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T07:23:41.927216Z","iopub.execute_input":"2024-12-25T07:23:41.927536Z","iopub.status.idle":"2024-12-25T07:27:12.392824Z","shell.execute_reply.started":"2024-12-25T07:23:41.927512Z","shell.execute_reply":"2024-12-25T07:27:12.392058Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 581ms/step\n\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 482ms/step\n\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 479ms/step\n[[    0.99981]\n [    0.73789]\n [   0.029843]\n ...\n [   0.079657]\n [    0.12422]\n [    0.42182]]\nPredictions saved yes.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"#simpan model\nmodel_save_dir = '/kaggle/working/ensemble_models'\nos.makedirs(model_save_dir, exist_ok=True)  # Create directory if it doesn't exist\n\nfor i, model in enumerate(models):\n    model.save(os.path.join(model_save_dir, f'modelsub_{i}.h5'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T06:13:25.685063Z","iopub.execute_input":"2024-12-25T06:13:25.685425Z","iopub.status.idle":"2024-12-25T06:13:32.979598Z","shell.execute_reply.started":"2024-12-25T06:13:25.685396Z","shell.execute_reply":"2024-12-25T06:13:32.978608Z"}},"outputs":[],"execution_count":19}]}